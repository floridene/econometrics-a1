---
title: "Assignment 1"
author: "Vera Weidmann, Marvin König, Sebastian Seck"
date: "5/22/2017"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#setwd("~/Documents/C Education/3 HWR/2. Semester SS2017/ECO")
```

```{r Initialization, warning=FALSE}
library(foreign) #to read in the .dta format
library(stargazer) #used in 1(c)-ii
hprice <- read.dta("/Users/marvin/Documents/Hochschule für Wirtschaft & Recht/Business Intelligence & Process Management/Econometrics/Data/hprice1.dta")
wage2 <- read.dta("/Users/marvin/Documents/Hochschule für Wirtschaft & Recht/Business Intelligence & Process Management/Econometrics/Data/WAGE2.DTA")
```

##Task 1
#### 1(a)
```{r 1a}
names(hprice)
summary(hprice[,c("price","bdrms","lotsize")])
```
The sample mean of price is 293.5. The summary statistics can be found above.

#### 1(b)
```{r 1b}
mod1b <- lm(lprice ~ bdrms + llotsize, data=hprice)
summary(mod1b)
```
The summary provides us with an R-Squared of 40.13% - this means that 40% of deviation in (log-) price can be explained by the predictors chosen. We are also provided with an adjusted R-squared, which is adjusted not to consider the count of variables used for the model.
n can be derived from the degrees of freedom: $DF = n-k-1 \Leftrightarrow 85 = 88-2-1$ From this formula we can easily deduct the number of observations n, which is 88.

If we consider that bdrms and lotsize is equal to 0, the price is the value of the intercept: 2.95. The model isa log-level-model. This means, an increase of one predictor by holding the others fixed leads to an percentage increase of the response varible (log-price).
So, if bdrms is increasing by one extra room while the other predictor lotsize is held fixed, the price increases by 14%. For one increase in log(lotsize) the price rise by 24.4%.

$\frac{\delta log(price)}{\delta bdrms}=\beta_1$
$\frac{\delta log(price)}{\delta log(lotsize)}=\beta_2$

Furthermore, both coefficients are significant. The p-vales of both coefficients are one indicator for the significance. As they are lower than $\alpha = 5%$ (and $\alpha = 1%$), we can reject that both predictors has no effect on the response price (H0: $\beta_1 = 0, \beta_2  =0$) in a two-sided test.
Likewise we could look at the t-values for signifcance. In R the significance codes give also a clarification about the coef. significance.

#### 1(c) - i
```{r 1ci}
#1(c) - i
hprice$sqllotsize <- hprice$llotsize^2 #to account for the additional variable, we simply create another column
mod1c <- lm(lprice ~ bdrms  + llotsize + sqllotsize, data = hprice)
summary(mod1c)
```
#### 1(c) - ii
```{r 1cii, results="asis"}
stargazer(mod1b,mod1c, title = "Comparison of models mod1b and mod1c", type="html", model.names = FALSE,column.labels = c("mod1b", "mod1c"), column.separate = c(1,1), style = "qje")
```

The first derivative of $log(price)=\beta_0+\beta_1bdrms+\beta_2log(lotsize)+\beta_3[log(lotsize)]^2+u$:  

$\frac{\delta log(price)}{\delta log(lotsize)}=\beta_2+2\beta_3log(lotsize)$

In model "mod1c" we add the squared value of llotsize. As it is shown in the derivation above, the effect from lotsize by one unit depends on how many units of lotsize is already there. So if... EXAMPLE!!!

#### 1(e)
At first we look at the variance of the error term of model 2, for which we square the Root MSE. In order to calculate the standard error of llotsize and bdrms we are using the SST and R-squared from the respective model outputs, where the variables are the dependent variable. We take the square root in order to arrive at the standard error instead of the variance.

In detail, this is how $SE_{\beta_2}$ (llotsize) is calculated:  
$SE_{\beta_2}=\sqrt{\frac{\sigma^2}{SST_2(1-R^2_2)}} = \sqrt{\frac{0.23^2}{25.75*(1-0.9939)}}=0.604$

In detail, this is how $SE_{\beta_1}$ (bdrms) is calculated:  
$SE_{\beta_1}=\sqrt{\frac{\sigma^2}{SST_1(1-R^2_1)}} = \sqrt{\frac{0.23^2}{61.59*(1-0.0289)}}=0.0308$

##Task 2
####2(a)
```{r 2a}
mod2a <- lm(lwage ~ educ + exper + tenure + married + black + south + urban, data = wage2)
summary(mod2a)

wage2$nonblack <- 1 - wage2$black

#90% Confidence Interval (alpha = 0.05 two-sided)
x = mod2a$coefficients["black"]
SE = sqrt(diag(vcov(mod2a)))["black"]
CI1= x - SE * 1.64
CI2 = x + SE * 1.64
(salary_black <-cbind(CI1, CI2))
```
The salary difference can be found from the coefficient $\beta_5$ in model mod2a, it is -0.1884. As the CI is not included in R's standard model output, we calculated it manually.

```{r 2b}
#Creating the additional variables
wage2$exper_sq <- wage2$exper^2
wage2$tenure_sq <- wage2$tenure^2

mod2b <- lm(lwage ~ educ + exper + tenure + married + black + south + urban + exper_sq + tenure_sq, data = wage2)
summary(mod2b)

#Calculating the residual sum of squares for both models
SSRr <- sum(mod2a$residuals^2)
SSRur <- sum(mod2b$residuals^2)

SSRr
SSRur
```
Our null hypothesis is that both additional variables have no explanatory power. 
$H_0: \beta_8=0; \beta_9=0$
$H_1: \beta_8>0; \beta_9>0$

Dicision Rule:
Reject H0 if $f > 1.114$ ($\alpha = 0.05$) -> Nochmal nachschauen!!!
```{r}
qf(0.95, df1=925, df2=927) #Wrong, Vera checks!!!
```


To calculate the F-statistic, we use the following formula:
$F = \frac{(SSR_r - SSR_{ur}) / q}{SSR_{ur} / (n - k - 1)}$

```{r}
q = 2
n = 935 
k= 9

(F_test=((SSRr-SSRur) / q)/(SSRur / (n-k-1)))
```

Substituted with our values:
$F = \frac{(123.8185 - 123.421) / 2}{123.421 / (935 - 9 - 1)} = 1.489806$

We reject our H0, because f > 1.114 and lies outside of the 95% Interval. This means $\beta8$ and $\beta9$ do provide additional explanation power. 

```{r}
anova(mod2a,mod2b)
```


2c)
```{r 2c}
mod2c <- lm(lwage ~ educ*black + exper + tenure + married + south + urban, data = wage2)
summary(mod2c)
````

$H0: \beta8 =0$
$H1: \beta8 != 0$ # sebastian fix!!! :) oder nur kleiner?

Decision Rule: Reject H0 if |t| > 1.64   ($\alpha = 0.1 $)

As t(educ:black) is |-1.121| < 1.64, we can not reject H0.


In model 2c we can see that the black coefficient is positive now (-0.18 --> 0.09). The standard error is quiet big, so that we can't verify a significance for this coefficient anymore. One assumption could be that, ???

2ci)
```{r 2ci, results="asis"}
stargazer(mod2a,mod2c, title = "Comparison of models mod2a and mod2c", type="html", model.names = FALSE,column.labels = c("mod2a", "mod2c"), column.separate = c(1,1), style = "qje")
```

2d)
```{r 2d}
mod2d <- lm(lwage ~ educ + black*married + exper + tenure + south + urban, data = wage2)
summary(mod2d)
````

Non_Married & Non_Black: Base Model
Non_Married & Black:beta2
Married & Non_Black:beta3
Married & Black:beta2, beta 3, beta8

Diff married non_blacks & married blacks:
beta2 + beta 8 = -0.24 + 0.06 = -0.18 (same like in mod2a)
